{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG Retrieval & Generation Evaluation\n",
        "\n",
        "This notebook offers a lightweight harness for evaluating retrieval quality and end-to-end responses.\n",
        "Run the setup cell, adjust the dataset path, and execute the evaluation helpers to inspect precision, MRR, and generated answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys, json\n",
        "\n",
        "# Add repository root to import path when running from the notebook directory\n",
        "REPO_ROOT = Path.cwd().resolve().parent\n",
        "if (REPO_ROOT / 'scripts').exists():\n",
        "    sys.path.append(str(REPO_ROOT))\n",
        "\n",
        "DATASET_PATH = REPO_ROOT / 'data' / 'samples' / 'queries.jsonl'\n",
        "print(f'Dataset path: {DATASET_PATH}')\n",
        "print('Ensure OpenSearch and Ollama are running before proceeding.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scripts.eval_retrieval import load_queries, ensure_retriever, evaluate_queries\n",
        "\n",
        "retriever = ensure_retriever(index_name='quest-research')\n",
        "queries = load_queries(DATASET_PATH)\n",
        "metrics = evaluate_queries(retriever, queries, top_k=5)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect Generated Answers\n",
        "\n",
        "Use the smoke-test helper to inspect LLM outputs for a specific question.\n",
        "Adjust `question` and `model_override` to target different queries or models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scripts.smoke_test import smoke_test\n",
        "\n",
        "smoke_test(\n",
        "    pdf_path=None,\n",
        "    question='Summarize the key findings about attention mechanisms.',\n",
        "    index_name='quest-research',\n",
        "    model_override='mistral',\n",
        "    timeout_override=120.0,\n",
        "    fallback_override='gemma3:1b',\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}